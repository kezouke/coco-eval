{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121c32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kezouke/Thesis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "your_hf_token = \"\"\n",
    "\n",
    "login(token=your_hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca8adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"llava-hf/llava-1.5-7b-hf\"\n",
    "COCO_DIR = \"Thesis/coco2014\"\n",
    "IMAGE_DIR = os.path.join(COCO_DIR, \"val2014\")\n",
    "ANNOTATION_FILE = os.path.join(COCO_DIR, \"annotations\", \"captions_val2014.json\")\n",
    "OUTPUT_FILE = \"llava_v1.5_7b_coco_results.json\"\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e76cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation parameters\n",
    "GENERATION_PARAMS = {\n",
    "    \"max_new_tokens\": 4,\n",
    "    \"do_sample\": False,  # Use greedy decoding\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa9874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free GPU memory: 23.26 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Free GPU memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb23ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_processor(model_path, device):\n",
    "    \"\"\"Load LLaVA model and processor.\"\"\"\n",
    "    print(f\"Loading model: {model_path}\")\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        model_path,\n",
    "        dtype=torch.float16,\n",
    "        device_map=device\n",
    "    ).eval()\n",
    "    model = torch.compile(model)\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "    print(f\"✓ Model loaded successfully on {device}\")\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da03114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_conversation(prompt=\"Describe this image\"):\n",
    "    \"\"\"Prepare conversation template for LLaVA.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80881703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions_batch(image_paths, model, processor, batch_size=4, prompt=\"Describe this image.\"):\n",
    "    \"\"\"Generate captions for a batch of images using TRUE batching.\"\"\"\n",
    "    # Step 1: Load all images\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            # Use a dummy image or skip? For simplicity, append a blank (not ideal)\n",
    "            # Better: handle missing images upstream\n",
    "            images.append(Image.new(\"RGB\", (224, 224)))\n",
    "\n",
    "    # Step 2: Prepare conversation template for each image (same prompt)\n",
    "    conversations = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        for _ in images\n",
    "    ]\n",
    "\n",
    "    # Step 3: Apply chat template to get text prompts\n",
    "    prompt_texts = [\n",
    "        processor.apply_chat_template(conv, add_generation_prompt=True)\n",
    "        for conv in conversations\n",
    "    ]\n",
    "\n",
    "    # Step 4: Process ALL images + texts together\n",
    "    inputs = processor(\n",
    "        images=images,\n",
    "        text=prompt_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True  # Important for variable-length text\n",
    "    )\n",
    "\n",
    "    # Step 5: Move to GPU\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    if \"pixel_values\" in inputs:\n",
    "        inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(torch.float16)\n",
    "\n",
    "    # Step 6: Generate captions in one go\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=17,\n",
    "            do_sample=False,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Step 7: Decode all outputs\n",
    "    captions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Step 8: Post-process each caption\n",
    "    clean_captions = []\n",
    "    for caption in captions:\n",
    "        if \"Assistant:\" in caption:\n",
    "            caption = caption.split(\"Assistant:\")[-1].strip()\n",
    "        elif \"<|assistant|>\" in caption:\n",
    "            caption = caption.split(\"<|assistant|>\")[-1].strip()\n",
    "        clean_captions.append(caption)\n",
    "\n",
    "    return clean_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b69bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_coco(annotation_file, results_file):\n",
    "    \"\"\"Evaluate results using pycocoevalcap.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COCO Evaluation Results\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load COCO API\n",
    "    coco = COCO(annotation_file)\n",
    "    coco_results = coco.loadRes(results_file)\n",
    "    \n",
    "    # Create evaluator\n",
    "    coco_eval = COCOEvalCap(coco, coco_results)\n",
    "    \n",
    "    # Run evaluation\n",
    "    coco_eval.evaluate()\n",
    "    \n",
    "    # Print scores\n",
    "    print(\"\\nMetrics:\")\n",
    "    for metric, score in coco_eval.eval.items():\n",
    "        print(f\"  {metric:10s}: {score:.3f}\")\n",
    "    \n",
    "    return coco_eval.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa9874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: llava-hf/llava-1.5-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully on cuda\n"
     ]
    }
   ],
   "source": [
    "model, processor = load_model_and_processor(MODEL_PATH, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed465c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 10.09 GB available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cad9fd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading COCO annotations from /home/kezouke/Thesis/coco2014/annotations/captions_val2014.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "Total images: 40504\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading COCO annotations from {ANNOTATION_FILE}\")\n",
    "coco = COCO(ANNOTATION_FILE)\n",
    "img_ids = coco.getImgIds()\n",
    "print(f\"Total images: {len(img_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ff71652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating captions for 40504 images...\n",
      "Batch size: 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nGenerating captions for {len(img_ids)} images...\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b47d1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "image_paths_list = []\n",
    "\n",
    "# Prepare image paths\n",
    "for img_id in img_ids:\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    image_path = os.path.join(IMAGE_DIR, img_info['file_name'])\n",
    "    image_paths_list.append((img_id, image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33b0526c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USER:  \\nDescribe this image. ASSISTANT: The image features a man wearing a red helmet, sitting on a motorcycle', 'USER:  \\nDescribe this image. ASSISTANT: The image features a woman wearing a red shirt, standing in a kitchen and', 'USER:  \\nDescribe this image. ASSISTANT: The image features a young boy standing in a field, holding an umbrella to', 'USER:  \\nDescribe this image. ASSISTANT: The image features a young boy wearing headphones and sitting at a computer des', 'USER:  \\nDescribe this image. ASSISTANT: The image features a group of people, including a young boy, sitting in front of', 'USER:  \\nDescribe this image. ASSISTANT: The image depicts a man standing in a large, well-equipped kitchen', 'USER:  \\nDescribe this image. ASSISTANT: The image features a woman standing in a kitchen, holding a cat in her arms.', 'USER:  \\nDescribe this image. ASSISTANT: The image features a young girl sitting at a dining table, enjoying a del']\n"
     ]
    }
   ],
   "source": [
    "test_paths = [image_paths_list[i][1] for i in range(8)]\n",
    "captions = generate_captions_batch(test_paths, model, processor, batch_size=8)\n",
    "print(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8dddb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 5063/5063 [2:28:45<00:00,  1.76s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Process in batches with progress bar\n",
    "for batch_idx in tqdm(range(0, len(image_paths_list), BATCH_SIZE), \n",
    "                        desc=\"Processing batches\"):\n",
    "    batch_data = image_paths_list[batch_idx:batch_idx+BATCH_SIZE]\n",
    "    img_ids_batch = [x[0] for x in batch_data]\n",
    "    paths_batch = [x[1] for x in batch_data]\n",
    "    \n",
    "    # Generate captions for batch\n",
    "    captions = generate_captions_batch(\n",
    "        paths_batch, \n",
    "        model, \n",
    "        processor, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        prompt=\"Describe this image.\"\n",
    "    )\n",
    "    \n",
    "    # Add to results\n",
    "    for img_id, caption in zip(img_ids_batch, captions):\n",
    "        results.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"caption\": caption\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a570de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to llava_v1.5_7b_coco_results.json\n",
      "✓ Results saved\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving results to {OUTPUT_FILE}\")\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(results, f)\n",
    "print(\"✓ Results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca125fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"llava_v1.5_7b_coco_results.json\"\n",
    "output_file = \"llava_v1.5_7b_coco_results_cleaned.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3f6ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned captions saved to llava_v1.5_7b_coco_results_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(input_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "cleaned_data = []\n",
    "for item in data:\n",
    "    caption = item.get(\"caption\", \"\")\n",
    "    if \"ASSISTANT:\" in caption:\n",
    "        caption = caption.split(\"ASSISTANT:\")[-1].strip()\n",
    "    elif \"Assistant:\" in caption:\n",
    "        caption = caption.split(\"Assistant:\")[-1].strip()\n",
    "    elif \"<|assistant|>\" in caption:\n",
    "        caption = caption.split(\"<|assistant|>\")[-1].strip()\n",
    "    caption = caption.strip()\n",
    "    cleaned_data.append({\n",
    "        \"image_id\": item[\"image_id\"],\n",
    "        \"caption\": caption\n",
    "    })\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(cleaned_data, f, indent=2)\n",
    "\n",
    "print(f\"Cleaned captions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3347016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COCO Evaluation Results\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with COCO metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COCO Evaluation Results\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "004fce2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 2492309 tokens at 2852716.61 tokens per second.\n",
      "PTBTokenizer tokenized 637477 tokens at 2538436.23 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "clipscore is using cuda\n",
      "computing Bleu score...\n",
      "{'testlen': 564095, 'reflen': 494937, 'guess': [564095, 523591, 483087, 442583], 'correct': [327108, 147333, 65341, 27919]}\n",
      "ratio: 1.139730915247797\n",
      "Bleu_1: 0.580\n",
      "Bleu_2: 0.404\n",
      "Bleu_3: 0.281\n",
      "Bleu_4: 0.193\n",
      "computing METEOR score...\n",
      "METEOR: 0.250\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.452\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.551\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/home/kezouke/Thesis/.venv/lib/python3.12/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [2.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [55.583 seconds]\n",
      "Threads( StanfordCoreNLP ) [52.443 seconds]\n",
      "Threads( StanfordCoreNLP ) [50.477 seconds]\n",
      "Threads( StanfordCoreNLP ) [50.661 seconds]\n",
      "Threads( StanfordCoreNLP ) [2.550 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 4.398 min\n",
      "SPICE: 0.182\n",
      "computing CLIPScore score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/159 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 159/159 [00:13<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to a numerical instability, new numpy normalization is slightly different than paper results. to exactly replicate paper results, please use numpy version less than 1.21, e.g., 1.20.3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/792 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 792/792 [00:38<00:00, 20.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to a numerical instability, new numpy normalization is slightly different than paper results. to exactly replicate paper results, please use numpy version less than 1.21, e.g., 1.20.3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40504it [00:00, 122859.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPScore: 0.761\n",
      "RefCLIPScore: 0.806\n"
     ]
    }
   ],
   "source": [
    "coco_results = coco.loadRes(output_file)\n",
    "coco_eval = COCOEvalCap(coco, coco_results)\n",
    "coco_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cca8f13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics:\n",
      "  Bleu_1    : 0.5799\n",
      "  Bleu_2    : 0.4039\n",
      "  Bleu_3    : 0.2805\n",
      "  Bleu_4    : 0.1932\n",
      "  METEOR    : 0.2503\n",
      "  ROUGE_L   : 0.4515\n",
      "  CIDEr     : 0.5507\n",
      "  SPICE     : 0.1819\n",
      "  CLIPScore : 0.7612\n",
      "  RefCLIPScore: 0.8057\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMetrics:\")\n",
    "for metric, score in coco_eval.eval.items():\n",
    "    print(f\"  {metric:10s}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c7dfeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Scores saved to llava_v1.5_7b_coco_results_scores.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_serializable(obj):\n",
    "    if isinstance(obj, np.generic):\n",
    "        return obj.item()  # converts np.float16, np.int32, etc. to Python scalar\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "scores_file = OUTPUT_FILE.replace('.json', '_scores.json')\n",
    "with open(scores_file, 'w') as f:\n",
    "    json.dump(coco_eval.eval, f, indent=2, default=to_serializable)\n",
    "\n",
    "print(f\"\\n✓ Scores saved to {scores_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
